{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ENV\\anaconda\\envs\\chat\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len c0:  4865  c1:  4703  c2:  12248  c3:  4404\n",
      "load  0  /  26220\n",
      "load  1000  /  26220\n",
      "load  2000  /  26220\n",
      "load  3000  /  26220\n",
      "load  4000  /  26220\n",
      "load  5000  /  26220\n",
      "load  6000  /  26220\n",
      "load  7000  /  26220\n",
      "load  8000  /  26220\n",
      "load  9000  /  26220\n",
      "load  10000  /  26220\n",
      "load  11000  /  26220\n",
      "load  12000  /  26220\n",
      "load  13000  /  26220\n",
      "load  14000  /  26220\n",
      "load  15000  /  26220\n",
      "load  16000  /  26220\n",
      "load  17000  /  26220\n",
      "load  18000  /  26220\n",
      "load  19000  /  26220\n",
      "load  20000  /  26220\n",
      "load  21000  /  26220\n",
      "load  22000  /  26220\n",
      "load  23000  /  26220\n",
      "load  24000  /  26220\n",
      "load  25000  /  26220\n",
      "load  26000  /  26220\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = Path(\"datasets/raw\")\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class SkillDataset(Dataset):\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        super().__init__()\n",
    "        all_img = list(dataset_path.glob(\"*.png\"));\n",
    "        self.c0 = [p for p in all_img if p.stem.endswith(\"_0\")]\n",
    "        self.c1 = [p for p in all_img if p.stem.endswith(\"_1\")]\n",
    "        self.c2 = [p for p in all_img if p.stem.endswith(\"_2\")]\n",
    "        self.c3 = [p for p in all_img if p.stem.endswith(\"_3\")]\n",
    "        print(\"len c0: \", len(self.c0), \" c1: \", len(self.c1), \" c2: \", len(self.c2), \" c3: \", len(self.c3))\n",
    "\n",
    "        self.loader = default_loader\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "            transforms.RandomPosterize(3),\n",
    "            transforms.RandomAdjustSharpness(3),\n",
    "            transforms.RandomAutocontrast(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        # 图片没多大，一次性全部载入内存算了\n",
    "        self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.c0) + len(self.c1) + len(self.c2) + len(self.c3)\n",
    "    \n",
    "    def get(self, index):\n",
    "        if index % 1000 == 0:\n",
    "            print(\"load \", index, \" / \", len(self))\n",
    "        if index < len(self.c0):\n",
    "            return self.transform(self.loader(self.c0[index])), 0\n",
    "        elif index < len(self.c0) + len(self.c1):\n",
    "            return self.transform(self.loader(self.c1[index - len(self.c0)])), 1\n",
    "        elif index < len(self.c0) + len(self.c1) + len(self.c2):\n",
    "            return self.transform(self.loader(self.c2[index - len(self.c0) - len(self.c1)])), 2\n",
    "        else:\n",
    "            return self.transform(self.loader(self.c3[index - len(self.c0) - len(self.c1) - len(self.c2)])), 3\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "        \n",
    "\n",
    "dataset = SkillDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 显存不够可以把 batch size 改小点\n",
    "train_loader = DataLoader(train_dataset, batch_size=3072, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_ch) -> None:\n",
    "        super().__init__()\n",
    "        self.branch_1x1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "\n",
    "        self.branch_5x5_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch_3x3_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = torch.nn.Conv2d(in_ch, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_1x1 = self.branch_1x1(x)\n",
    "\n",
    "        branch_5x5 = self.branch_5x5_1(x)\n",
    "        branch_5x5 = self.branch_5x5_2(branch_5x5)\n",
    "\n",
    "        branch_3x3 = self.branch_3x3_1(x)\n",
    "        branch_3x3 = self.branch_3x3_2(branch_3x3)\n",
    "        branch_3x3 = self.branch_3x3_3(branch_3x3)\n",
    "\n",
    "        branch_pool = torch.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch_1x1, branch_5x5, branch_3x3, branch_pool] # 16 + 24 + 24 + 24\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class GoogleNet(torch.nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(channels, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(88, 20, kernel_size=5)\n",
    "        self.incep1 = InceptionA(10)\n",
    "        self.incep2 = InceptionA(20)\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(38808, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incep1(x)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if not use_cuda:\n",
    "    print(\"WARNING: CPU will be used for training.\")\n",
    "\n",
    "model = GoogleNet(3).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def train(epoch):\n",
    "    global start_time\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    cur_time = time.time()\n",
    "    cost = cur_time - start_time\n",
    "    print(f'Train Epoch: {epoch}, Loss: {loss.item():.8f}, cost: {cost:.2f} s')\n",
    "    start_time = cur_time\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'=== Test: Loss: {test_loss:.8f}, Acc: {acc:.4f} ===')\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Loss: 1.60028446, cost: 5.70 s\n",
      "=== Test: Loss: 0.00319802, Acc: 45.5759 ===\n",
      "=== Pre best is 0, Loss: 100.00000000, Acc: 0.0000 ===\n",
      "====== New best is 0, Loss: 0.00319802, Acc: 45.5759 ======\n",
      "Train Epoch: 1, Loss: 1.28577912, cost: 3.73 s\n",
      "Train Epoch: 2, Loss: 1.28855324, cost: 3.34 s\n",
      "Train Epoch: 3, Loss: 1.25775635, cost: 3.35 s\n",
      "Train Epoch: 4, Loss: 1.24128604, cost: 3.35 s\n",
      "Train Epoch: 5, Loss: 1.13937271, cost: 3.36 s\n",
      "Train Epoch: 6, Loss: 1.02353418, cost: 3.36 s\n",
      "Train Epoch: 7, Loss: 0.84396064, cost: 3.37 s\n",
      "Train Epoch: 8, Loss: 0.58244020, cost: 3.39 s\n",
      "Train Epoch: 9, Loss: 0.50752330, cost: 3.39 s\n",
      "Train Epoch: 10, Loss: 0.45972630, cost: 3.37 s\n",
      "=== Test: Loss: 0.00098966, Acc: 81.9222 ===\n",
      "=== Pre best is 0, Loss: 0.00319802, Acc: 45.5759 ===\n",
      "====== New best is 10, Loss: 0.00098966, Acc: 81.9222 ======\n",
      "Train Epoch: 11, Loss: 0.43363693, cost: 3.77 s\n",
      "Train Epoch: 12, Loss: 0.40060765, cost: 3.38 s\n",
      "Train Epoch: 13, Loss: 0.36842823, cost: 3.38 s\n",
      "Train Epoch: 14, Loss: 0.33153898, cost: 3.43 s\n",
      "Train Epoch: 15, Loss: 0.32253763, cost: 3.38 s\n",
      "Train Epoch: 16, Loss: 0.27589989, cost: 3.38 s\n",
      "Train Epoch: 17, Loss: 0.29258406, cost: 3.39 s\n",
      "Train Epoch: 18, Loss: 0.27483019, cost: 3.39 s\n",
      "Train Epoch: 19, Loss: 0.25969380, cost: 3.39 s\n",
      "Train Epoch: 20, Loss: 0.23989818, cost: 3.40 s\n",
      "=== Test: Loss: 0.00056927, Acc: 90.7323 ===\n",
      "=== Pre best is 10, Loss: 0.00098966, Acc: 81.9222 ===\n",
      "====== New best is 20, Loss: 0.00056927, Acc: 90.7323 ======\n",
      "Train Epoch: 21, Loss: 0.21536522, cost: 3.79 s\n",
      "Train Epoch: 22, Loss: 0.23232768, cost: 3.40 s\n",
      "Train Epoch: 23, Loss: 0.22484584, cost: 3.40 s\n",
      "Train Epoch: 24, Loss: 0.20339030, cost: 3.40 s\n",
      "Train Epoch: 25, Loss: 0.22295399, cost: 3.40 s\n",
      "Train Epoch: 26, Loss: 0.20760024, cost: 3.40 s\n",
      "Train Epoch: 27, Loss: 0.19208339, cost: 3.40 s\n",
      "Train Epoch: 28, Loss: 0.18629681, cost: 3.40 s\n",
      "Train Epoch: 29, Loss: 0.18581350, cost: 3.40 s\n",
      "Train Epoch: 30, Loss: 0.19244698, cost: 3.40 s\n",
      "=== Test: Loss: 0.00035642, Acc: 94.1457 ===\n",
      "=== Pre best is 20, Loss: 0.00056927, Acc: 90.7323 ===\n",
      "====== New best is 30, Loss: 0.00035642, Acc: 94.1457 ======\n",
      "Train Epoch: 31, Loss: 0.14557877, cost: 3.80 s\n",
      "Train Epoch: 32, Loss: 0.14847375, cost: 3.40 s\n",
      "Train Epoch: 33, Loss: 0.13617074, cost: 3.44 s\n",
      "Train Epoch: 34, Loss: 0.11962784, cost: 3.41 s\n",
      "Train Epoch: 35, Loss: 0.12918045, cost: 3.41 s\n",
      "Train Epoch: 36, Loss: 0.11382781, cost: 3.41 s\n",
      "Train Epoch: 37, Loss: 0.10005002, cost: 3.40 s\n",
      "Train Epoch: 38, Loss: 0.11872751, cost: 3.42 s\n",
      "Train Epoch: 39, Loss: 0.08999947, cost: 3.42 s\n",
      "Train Epoch: 40, Loss: 0.07632575, cost: 3.41 s\n",
      "=== Test: Loss: 0.00020485, Acc: 96.6819 ===\n",
      "=== Pre best is 30, Loss: 0.00035642, Acc: 94.1457 ===\n",
      "====== New best is 40, Loss: 0.00020485, Acc: 96.6819 ======\n",
      "Train Epoch: 41, Loss: 0.08536188, cost: 3.81 s\n",
      "Train Epoch: 42, Loss: 0.07126414, cost: 3.41 s\n",
      "Train Epoch: 43, Loss: 0.06825692, cost: 3.41 s\n",
      "Train Epoch: 44, Loss: 0.06598806, cost: 3.42 s\n",
      "Train Epoch: 45, Loss: 0.05085293, cost: 3.41 s\n",
      "Train Epoch: 46, Loss: 0.04792800, cost: 3.41 s\n",
      "Train Epoch: 47, Loss: 0.05764360, cost: 3.42 s\n",
      "Train Epoch: 48, Loss: 0.03957058, cost: 3.41 s\n",
      "Train Epoch: 49, Loss: 0.03618916, cost: 3.41 s\n",
      "Train Epoch: 50, Loss: 0.03815326, cost: 3.42 s\n",
      "=== Test: Loss: 0.00013816, Acc: 98.1121 ===\n",
      "=== Pre best is 40, Loss: 0.00020485, Acc: 96.6819 ===\n",
      "====== New best is 50, Loss: 0.00013816, Acc: 98.1121 ======\n",
      "Train Epoch: 51, Loss: 0.03883927, cost: 3.81 s\n",
      "Train Epoch: 52, Loss: 0.03492482, cost: 3.41 s\n",
      "Train Epoch: 53, Loss: 0.02618423, cost: 3.41 s\n",
      "Train Epoch: 54, Loss: 0.02763427, cost: 3.41 s\n",
      "Train Epoch: 55, Loss: 0.02109393, cost: 3.42 s\n",
      "Train Epoch: 56, Loss: 0.02603738, cost: 3.42 s\n",
      "Train Epoch: 57, Loss: 0.01921456, cost: 3.41 s\n",
      "Train Epoch: 58, Loss: 0.01442107, cost: 3.42 s\n",
      "Train Epoch: 59, Loss: 0.01842108, cost: 3.41 s\n",
      "Train Epoch: 60, Loss: 0.00967526, cost: 3.42 s\n",
      "=== Test: Loss: 0.00011718, Acc: 98.8749 ===\n",
      "=== Pre best is 50, Loss: 0.00013816, Acc: 98.1121 ===\n",
      "====== New best is 60, Loss: 0.00011718, Acc: 98.8749 ======\n",
      "Train Epoch: 61, Loss: 0.01506887, cost: 3.81 s\n",
      "Train Epoch: 62, Loss: 0.00639584, cost: 3.42 s\n",
      "Train Epoch: 63, Loss: 0.00802180, cost: 3.42 s\n",
      "Train Epoch: 64, Loss: 0.00760378, cost: 3.42 s\n",
      "Train Epoch: 65, Loss: 0.01021129, cost: 3.41 s\n",
      "Train Epoch: 66, Loss: 0.00717891, cost: 3.48 s\n",
      "Train Epoch: 67, Loss: 0.00434920, cost: 3.42 s\n",
      "Train Epoch: 68, Loss: 0.00475077, cost: 3.42 s\n",
      "Train Epoch: 69, Loss: 0.00338338, cost: 3.42 s\n",
      "Train Epoch: 70, Loss: 0.00290226, cost: 3.43 s\n",
      "=== Test: Loss: 0.00014121, Acc: 98.9512 ===\n",
      "=== Pre best is 60, Loss: 0.00011718, Acc: 98.8749 ===\n",
      "Train Epoch: 71, Loss: 0.00335653, cost: 3.82 s\n",
      "Train Epoch: 72, Loss: 0.00260121, cost: 3.42 s\n",
      "Train Epoch: 73, Loss: 0.00116879, cost: 3.42 s\n",
      "Train Epoch: 74, Loss: 0.00212963, cost: 3.43 s\n",
      "Train Epoch: 75, Loss: 0.00128581, cost: 3.42 s\n",
      "Train Epoch: 76, Loss: 0.00122046, cost: 3.42 s\n",
      "Train Epoch: 77, Loss: 0.00120911, cost: 3.44 s\n",
      "Train Epoch: 78, Loss: 0.00165543, cost: 3.43 s\n",
      "Train Epoch: 79, Loss: 0.00117700, cost: 3.48 s\n",
      "Train Epoch: 80, Loss: 0.00090678, cost: 3.42 s\n",
      "=== Test: Loss: 0.00016802, Acc: 98.9893 ===\n",
      "=== Pre best is 60, Loss: 0.00011718, Acc: 98.8749 ===\n",
      "Train Epoch: 81, Loss: 0.00108119, cost: 3.82 s\n",
      "Train Epoch: 82, Loss: 0.00089823, cost: 3.43 s\n",
      "Train Epoch: 83, Loss: 0.00065432, cost: 3.43 s\n",
      "Train Epoch: 84, Loss: 0.00053204, cost: 3.44 s\n",
      "Train Epoch: 85, Loss: 0.00045595, cost: 3.45 s\n",
      "Train Epoch: 86, Loss: 0.00045855, cost: 3.44 s\n",
      "Train Epoch: 87, Loss: 0.00041063, cost: 3.43 s\n",
      "Train Epoch: 88, Loss: 0.00048473, cost: 3.43 s\n",
      "Train Epoch: 89, Loss: 0.00057961, cost: 3.43 s\n",
      "Train Epoch: 90, Loss: 0.00065771, cost: 3.42 s\n",
      "=== Test: Loss: 0.00019212, Acc: 99.0084 ===\n",
      "=== Pre best is 60, Loss: 0.00011718, Acc: 98.8749 ===\n",
      "Train Epoch: 91, Loss: 0.00039065, cost: 3.83 s\n",
      "Train Epoch: 92, Loss: 0.00037425, cost: 3.43 s\n",
      "Train Epoch: 93, Loss: 0.00023381, cost: 3.42 s\n",
      "Train Epoch: 94, Loss: 0.00030654, cost: 3.43 s\n",
      "Train Epoch: 95, Loss: 0.00025788, cost: 3.43 s\n",
      "Train Epoch: 96, Loss: 0.00023536, cost: 3.42 s\n",
      "Train Epoch: 97, Loss: 0.00031656, cost: 3.42 s\n",
      "Train Epoch: 98, Loss: 0.00024969, cost: 3.43 s\n",
      "Train Epoch: 99, Loss: 0.00023206, cost: 3.43 s\n",
      "Train Epoch: 100, Loss: 0.00014972, cost: 3.43 s\n",
      "=== Test: Loss: 0.00020600, Acc: 98.9893 ===\n",
      "=== Pre best is 60, Loss: 0.00011718, Acc: 98.8749 ===\n",
      "Train Epoch: 101, Loss: 0.00017079, cost: 3.83 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m====== New best is \u001b[39m\u001b[39m{\u001b[39;00mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mbest_loss\u001b[39m:\u001b[39;00m\u001b[39m.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Acc: \u001b[39m\u001b[39m{\u001b[39;00mbest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ======\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m         torch\u001b[39m.\u001b[39msave(model, best_model_path)\n\u001b[1;32m---> 38\u001b[0m pipeline()\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(start_epoch, test_interval)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mglobal\u001b[39;00m best_epoch, best_loss, best_acc\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, \u001b[39m1000\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     train(epoch)\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m test_interval \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 8\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "output = Path('checkpoints')\n",
    "output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_name = model.__class__.__name__\n",
    "best_model_path = output / f'{model_name}_best.pt'\n",
    "\n",
    "best_epoch = 0\n",
    "best_loss = 100.0\n",
    "best_acc = 0.0\n",
    "default_interval = 10\n",
    "\n",
    "def pipeline(start_epoch = 0, test_interval = default_interval):\n",
    "    global best_epoch, best_loss, best_acc\n",
    "\n",
    "    for epoch in range(start_epoch, 1000):\n",
    "        train(epoch)\n",
    "        if epoch % test_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        loss, acc = test()\n",
    "        print(f'=== Pre best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ===')\n",
    "        torch.save(model, output / f'{model_name}_{epoch}.pt')\n",
    "        if loss > best_loss:\n",
    "            if epoch - best_epoch > test_interval * 10:\n",
    "                print('No improvement for a long time, Early stop!')\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        best_epoch = epoch\n",
    "        best_loss = loss\n",
    "        best_acc = acc\n",
    "        print(f'====== New best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ======')\n",
    "        torch.save(model, best_model_path)\n",
    "\n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: Loss: 0.00011718, Acc: 98.8749 ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00011718434812309267, 98.87490465293669)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(best_model_path)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出 onnx\n",
    "\n",
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def convert_onnx(path: Path):\n",
    "    model = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 96, 96)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        path.with_suffix(\".onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "convert_onnx(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.checker.check_model(str(best_model_path.with_suffix(\".onnx\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
