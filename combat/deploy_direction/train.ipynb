{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ENV\\anaconda\\envs\\chat\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len c0:  4865  c1:  4703  c2:  12248  c3:  4404\n",
      "load  0  /  26220\n",
      "load  1000  /  26220\n",
      "load  2000  /  26220\n",
      "load  3000  /  26220\n",
      "load  4000  /  26220\n",
      "load  5000  /  26220\n",
      "load  6000  /  26220\n",
      "load  7000  /  26220\n",
      "load  8000  /  26220\n",
      "load  9000  /  26220\n",
      "load  10000  /  26220\n",
      "load  11000  /  26220\n",
      "load  12000  /  26220\n",
      "load  13000  /  26220\n",
      "load  14000  /  26220\n",
      "load  15000  /  26220\n",
      "load  16000  /  26220\n",
      "load  17000  /  26220\n",
      "load  18000  /  26220\n",
      "load  19000  /  26220\n",
      "load  20000  /  26220\n",
      "load  21000  /  26220\n",
      "load  22000  /  26220\n",
      "load  23000  /  26220\n",
      "load  24000  /  26220\n",
      "load  25000  /  26220\n",
      "load  26000  /  26220\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = Path(\"datasets/raw\")\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class SkillDataset(Dataset):\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        super().__init__()\n",
    "        all_img = list(dataset_path.glob(\"*.png\"));\n",
    "        self.c0 = [p for p in all_img if p.stem.endswith(\"_0\")]\n",
    "        self.c1 = [p for p in all_img if p.stem.endswith(\"_1\")]\n",
    "        self.c2 = [p for p in all_img if p.stem.endswith(\"_2\")]\n",
    "        self.c3 = [p for p in all_img if p.stem.endswith(\"_3\")]\n",
    "        print(\"len c0: \", len(self.c0), \" c1: \", len(self.c1), \" c2: \", len(self.c2), \" c3: \", len(self.c3))\n",
    "\n",
    "        self.loader = default_loader\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "            transforms.RandomPosterize(3),\n",
    "            transforms.RandomAdjustSharpness(3),\n",
    "            transforms.RandomAutocontrast(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        # 图片没多大，一次性全部载入内存算了\n",
    "        self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.c0) + len(self.c1) + len(self.c2) + len(self.c3)\n",
    "    \n",
    "    def get(self, index):\n",
    "        if index % 1000 == 0:\n",
    "            print(\"load \", index, \" / \", len(self))\n",
    "        if index < len(self.c0):\n",
    "            return self.transform(self.loader(self.c0[index])), 0\n",
    "        elif index < len(self.c0) + len(self.c1):\n",
    "            return self.transform(self.loader(self.c1[index - len(self.c0)])), 1\n",
    "        elif index < len(self.c0) + len(self.c1) + len(self.c2):\n",
    "            return self.transform(self.loader(self.c2[index - len(self.c0) - len(self.c1)])), 2\n",
    "        else:\n",
    "            return self.transform(self.loader(self.c3[index - len(self.c0) - len(self.c1) - len(self.c2)])), 3\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "        \n",
    "\n",
    "dataset = SkillDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 显存不够可以把 batch size 改小点\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelM5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelM5, self).__init__()\n",
    "        self.channels = 3\n",
    "        self.classes = 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.channels, 32, 5, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 96, 5, bias=False)\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        self.conv4 = nn.Conv2d(96, 128, 5, bias=False)\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 160, 5, bias=False)\n",
    "        self.conv5_bn = nn.BatchNorm2d(160)\n",
    "        self.fc1 = nn.Linear(924160, self.classes, bias=False)\n",
    "        self.fc1_bn = nn.BatchNorm1d(self.classes)\n",
    "    def get_logits(self, x):\n",
    "        x = (x - 0.5) * 2.0\n",
    "        conv1 = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        conv2 = F.relu(self.conv2_bn(self.conv2(conv1)))\n",
    "        conv3 = F.relu(self.conv3_bn(self.conv3(conv2)))\n",
    "        conv4 = F.relu(self.conv4_bn(self.conv4(conv3)))\n",
    "        conv5 = F.relu(self.conv5_bn(self.conv5(conv4)))\n",
    "        flat5 = torch.flatten(conv5.permute(0, 2, 3, 1), 1)\n",
    "        logits = self.fc1_bn(self.fc1(flat5))\n",
    "        return logits\n",
    "    def forward(self, x):\n",
    "        logits = self.get_logits(x)\n",
    "        return F.log_softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if not use_cuda:\n",
    "    print(\"WARNING: CPU will be used for training.\")\n",
    "\n",
    "model = ModelM5().to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def train(epoch):\n",
    "    global start_time\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    cur_time = time.time()\n",
    "    cost = cur_time - start_time\n",
    "    print(f'Train Epoch: {epoch}, Loss: {loss.item():.8f}, cost: {cost:.2f} s')\n",
    "    start_time = cur_time\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'=== Test: Loss: {test_loss:.8f}, Acc: {acc:.4f} ===')\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Loss: 0.41794336, cost: 38.50 s\n",
      "=== Test: Loss: 0.00663043, Acc: 96.8535 ===\n",
      "=== Pre best is 0, Loss: 100.00000000, Acc: 0.0000 ===\n",
      "====== New best is 0, Loss: 0.00663043, Acc: 96.8535 ======\n",
      "Train Epoch: 1, Loss: 0.33310065, cost: 35.56 s\n",
      "Train Epoch: 2, Loss: 0.30981356, cost: 32.08 s\n",
      "Train Epoch: 3, Loss: 0.27055177, cost: 32.24 s\n",
      "Train Epoch: 4, Loss: 0.23862752, cost: 32.18 s\n",
      "Train Epoch: 5, Loss: 0.20172793, cost: 32.14 s\n",
      "Train Epoch: 6, Loss: 0.18035056, cost: 32.18 s\n",
      "Train Epoch: 7, Loss: 0.17245628, cost: 32.06 s\n",
      "Train Epoch: 8, Loss: 0.14995044, cost: 32.07 s\n",
      "Train Epoch: 9, Loss: 0.14168449, cost: 32.15 s\n",
      "Train Epoch: 10, Loss: 0.12613584, cost: 32.19 s\n",
      "=== Test: Loss: 0.00206682, Acc: 99.6949 ===\n",
      "=== Pre best is 0, Loss: 0.00663043, Acc: 96.8535 ===\n",
      "====== New best is 10, Loss: 0.00206682, Acc: 99.6949 ======\n",
      "Train Epoch: 11, Loss: 0.11840280, cost: 35.47 s\n",
      "Train Epoch: 12, Loss: 0.10938285, cost: 32.12 s\n",
      "Train Epoch: 13, Loss: 0.10496280, cost: 32.12 s\n",
      "Train Epoch: 14, Loss: 0.09530918, cost: 32.11 s\n",
      "Train Epoch: 15, Loss: 0.08924822, cost: 32.09 s\n",
      "Train Epoch: 16, Loss: 0.08328618, cost: 32.10 s\n",
      "Train Epoch: 17, Loss: 0.08269164, cost: 32.12 s\n",
      "Train Epoch: 18, Loss: 0.07254216, cost: 32.10 s\n",
      "Train Epoch: 19, Loss: 0.06959699, cost: 32.10 s\n",
      "Train Epoch: 20, Loss: 0.06642153, cost: 32.19 s\n",
      "=== Test: Loss: 0.00113586, Acc: 99.7712 ===\n",
      "=== Pre best is 10, Loss: 0.00206682, Acc: 99.6949 ===\n",
      "====== New best is 20, Loss: 0.00113586, Acc: 99.7712 ======\n",
      "Train Epoch: 21, Loss: 0.06277595, cost: 35.45 s\n",
      "Train Epoch: 22, Loss: 0.06008631, cost: 32.10 s\n",
      "Train Epoch: 23, Loss: 0.05548321, cost: 32.17 s\n",
      "Train Epoch: 24, Loss: 0.05735600, cost: 32.14 s\n",
      "Train Epoch: 25, Loss: 0.05185343, cost: 32.10 s\n",
      "Train Epoch: 26, Loss: 0.05219206, cost: 32.10 s\n",
      "Train Epoch: 27, Loss: 0.04902924, cost: 32.07 s\n",
      "Train Epoch: 28, Loss: 0.04493771, cost: 32.13 s\n",
      "Train Epoch: 29, Loss: 0.04212640, cost: 32.06 s\n",
      "Train Epoch: 30, Loss: 0.04083553, cost: 32.11 s\n",
      "=== Test: Loss: 0.00089173, Acc: 99.8093 ===\n",
      "=== Pre best is 20, Loss: 0.00113586, Acc: 99.7712 ===\n",
      "====== New best is 30, Loss: 0.00089173, Acc: 99.8093 ======\n",
      "Train Epoch: 31, Loss: 0.03935941, cost: 35.37 s\n",
      "Train Epoch: 32, Loss: 0.03953591, cost: 32.07 s\n",
      "Train Epoch: 33, Loss: 0.03534453, cost: 32.16 s\n",
      "Train Epoch: 34, Loss: 0.03477651, cost: 32.21 s\n",
      "Train Epoch: 35, Loss: 0.03273365, cost: 32.23 s\n",
      "Train Epoch: 36, Loss: 0.03289957, cost: 32.20 s\n",
      "Train Epoch: 37, Loss: 0.03121678, cost: 32.08 s\n",
      "Train Epoch: 38, Loss: 0.02994918, cost: 32.09 s\n",
      "Train Epoch: 39, Loss: 0.02806101, cost: 32.09 s\n",
      "Train Epoch: 40, Loss: 0.02883939, cost: 32.08 s\n",
      "=== Test: Loss: 0.00054763, Acc: 99.8474 ===\n",
      "=== Pre best is 30, Loss: 0.00089173, Acc: 99.8093 ===\n",
      "====== New best is 40, Loss: 0.00054763, Acc: 99.8474 ======\n",
      "Train Epoch: 41, Loss: 0.02651981, cost: 35.43 s\n",
      "Train Epoch: 42, Loss: 0.02536817, cost: 32.13 s\n",
      "Train Epoch: 43, Loss: 0.02509535, cost: 32.16 s\n",
      "Train Epoch: 44, Loss: 0.02374115, cost: 32.18 s\n",
      "Train Epoch: 45, Loss: 0.02458679, cost: 32.12 s\n",
      "Train Epoch: 46, Loss: 0.02286425, cost: 32.12 s\n",
      "Train Epoch: 47, Loss: 0.02153459, cost: 32.11 s\n",
      "Train Epoch: 48, Loss: 0.02185869, cost: 32.09 s\n",
      "Train Epoch: 49, Loss: 0.02141124, cost: 32.13 s\n",
      "Train Epoch: 50, Loss: 0.02068084, cost: 32.11 s\n",
      "=== Test: Loss: 0.00045546, Acc: 99.8284 ===\n",
      "=== Pre best is 40, Loss: 0.00054763, Acc: 99.8474 ===\n",
      "====== New best is 50, Loss: 0.00045546, Acc: 99.8284 ======\n",
      "Train Epoch: 51, Loss: 0.01889266, cost: 35.45 s\n",
      "Train Epoch: 52, Loss: 0.01847451, cost: 32.13 s\n",
      "Train Epoch: 53, Loss: 0.01760967, cost: 32.22 s\n",
      "Train Epoch: 54, Loss: 0.01749671, cost: 32.14 s\n",
      "Train Epoch: 55, Loss: 0.01649523, cost: 32.11 s\n",
      "Train Epoch: 56, Loss: 0.01582577, cost: 32.13 s\n",
      "Train Epoch: 57, Loss: 0.01548098, cost: 32.13 s\n",
      "Train Epoch: 58, Loss: 0.01574891, cost: 32.12 s\n",
      "Train Epoch: 59, Loss: 0.01469435, cost: 32.12 s\n",
      "Train Epoch: 60, Loss: 0.01485459, cost: 32.14 s\n",
      "=== Test: Loss: 0.00036152, Acc: 99.8284 ===\n",
      "=== Pre best is 50, Loss: 0.00045546, Acc: 99.8284 ===\n",
      "====== New best is 60, Loss: 0.00036152, Acc: 99.8284 ======\n",
      "Train Epoch: 61, Loss: 0.01379515, cost: 35.49 s\n",
      "Train Epoch: 62, Loss: 0.01355618, cost: 32.15 s\n",
      "Train Epoch: 63, Loss: 0.01368057, cost: 32.14 s\n",
      "Train Epoch: 64, Loss: 0.01296321, cost: 32.16 s\n",
      "Train Epoch: 65, Loss: 0.01292810, cost: 32.14 s\n",
      "Train Epoch: 66, Loss: 0.01206731, cost: 32.13 s\n",
      "Train Epoch: 67, Loss: 0.01179258, cost: 32.15 s\n",
      "Train Epoch: 68, Loss: 0.01168768, cost: 32.15 s\n",
      "Train Epoch: 69, Loss: 0.01185841, cost: 32.19 s\n",
      "Train Epoch: 70, Loss: 0.01146408, cost: 32.16 s\n",
      "=== Test: Loss: 0.00030010, Acc: 99.8474 ===\n",
      "=== Pre best is 60, Loss: 0.00036152, Acc: 99.8284 ===\n",
      "====== New best is 70, Loss: 0.00030010, Acc: 99.8474 ======\n",
      "Train Epoch: 71, Loss: 0.01063056, cost: 35.48 s\n",
      "Train Epoch: 72, Loss: 0.01116208, cost: 32.18 s\n",
      "Train Epoch: 73, Loss: 0.01087694, cost: 32.22 s\n",
      "Train Epoch: 74, Loss: 0.01186743, cost: 32.17 s\n",
      "Train Epoch: 75, Loss: 0.00958546, cost: 32.22 s\n",
      "Train Epoch: 76, Loss: 0.00929295, cost: 32.22 s\n",
      "Train Epoch: 77, Loss: 0.00899134, cost: 32.20 s\n",
      "Train Epoch: 78, Loss: 0.00887060, cost: 32.18 s\n",
      "Train Epoch: 79, Loss: 0.00897730, cost: 32.21 s\n",
      "Train Epoch: 80, Loss: 0.00829251, cost: 32.13 s\n",
      "=== Test: Loss: 0.00025891, Acc: 99.8474 ===\n",
      "=== Pre best is 70, Loss: 0.00030010, Acc: 99.8474 ===\n",
      "====== New best is 80, Loss: 0.00025891, Acc: 99.8474 ======\n",
      "Train Epoch: 81, Loss: 0.00838887, cost: 35.49 s\n",
      "Train Epoch: 82, Loss: 0.00787651, cost: 32.15 s\n",
      "Train Epoch: 83, Loss: 0.00779882, cost: 32.19 s\n",
      "Train Epoch: 84, Loss: 0.00744510, cost: 32.16 s\n",
      "Train Epoch: 85, Loss: 0.00733222, cost: 32.16 s\n",
      "Train Epoch: 86, Loss: 0.00704737, cost: 32.17 s\n",
      "Train Epoch: 87, Loss: 0.00695762, cost: 32.19 s\n",
      "Train Epoch: 88, Loss: 0.00677532, cost: 32.17 s\n",
      "Train Epoch: 89, Loss: 0.00679515, cost: 32.17 s\n",
      "Train Epoch: 90, Loss: 0.00646702, cost: 32.18 s\n",
      "=== Test: Loss: 0.00022076, Acc: 99.8474 ===\n",
      "=== Pre best is 80, Loss: 0.00025891, Acc: 99.8474 ===\n",
      "====== New best is 90, Loss: 0.00022076, Acc: 99.8474 ======\n",
      "Train Epoch: 91, Loss: 0.00640386, cost: 35.58 s\n",
      "Train Epoch: 92, Loss: 0.00611191, cost: 32.24 s\n",
      "Train Epoch: 93, Loss: 0.00591418, cost: 32.22 s\n",
      "Train Epoch: 94, Loss: 0.00601699, cost: 32.18 s\n",
      "Train Epoch: 95, Loss: 0.00748722, cost: 32.18 s\n",
      "Train Epoch: 96, Loss: 0.00550311, cost: 32.21 s\n",
      "Train Epoch: 97, Loss: 0.00569690, cost: 32.24 s\n",
      "Train Epoch: 98, Loss: 0.00553758, cost: 32.17 s\n",
      "Train Epoch: 99, Loss: 0.00546730, cost: 32.19 s\n",
      "Train Epoch: 100, Loss: 0.00500236, cost: 32.19 s\n",
      "=== Test: Loss: 0.00019527, Acc: 99.8284 ===\n",
      "=== Pre best is 90, Loss: 0.00022076, Acc: 99.8474 ===\n",
      "====== New best is 100, Loss: 0.00019527, Acc: 99.8284 ======\n",
      "Train Epoch: 101, Loss: 0.00495299, cost: 35.53 s\n",
      "Train Epoch: 102, Loss: 0.00491342, cost: 32.17 s\n",
      "Train Epoch: 103, Loss: 0.00472554, cost: 32.19 s\n",
      "Train Epoch: 104, Loss: 0.00471163, cost: 32.19 s\n",
      "Train Epoch: 105, Loss: 0.00453710, cost: 32.21 s\n",
      "Train Epoch: 106, Loss: 0.00451089, cost: 32.24 s\n",
      "Train Epoch: 107, Loss: 0.00444697, cost: 32.18 s\n",
      "Train Epoch: 108, Loss: 0.00415109, cost: 32.16 s\n",
      "Train Epoch: 109, Loss: 0.00418804, cost: 32.21 s\n",
      "Train Epoch: 110, Loss: 0.00395953, cost: 32.20 s\n",
      "=== Test: Loss: 0.00017470, Acc: 99.8665 ===\n",
      "=== Pre best is 100, Loss: 0.00019527, Acc: 99.8284 ===\n",
      "====== New best is 110, Loss: 0.00017470, Acc: 99.8665 ======\n",
      "Train Epoch: 111, Loss: 0.00391987, cost: 35.54 s\n",
      "Train Epoch: 112, Loss: 0.00419787, cost: 32.19 s\n",
      "Train Epoch: 113, Loss: 0.00392190, cost: 32.19 s\n",
      "Train Epoch: 114, Loss: 0.00381483, cost: 32.18 s\n",
      "Train Epoch: 115, Loss: 0.00356501, cost: 32.20 s\n",
      "Train Epoch: 116, Loss: 0.00393731, cost: 32.17 s\n",
      "Train Epoch: 117, Loss: 0.00349789, cost: 32.19 s\n",
      "Train Epoch: 118, Loss: 0.00352798, cost: 32.19 s\n",
      "Train Epoch: 119, Loss: 0.00333063, cost: 32.20 s\n",
      "Train Epoch: 120, Loss: 0.00343219, cost: 32.17 s\n",
      "=== Test: Loss: 0.00015983, Acc: 99.8665 ===\n",
      "=== Pre best is 110, Loss: 0.00017470, Acc: 99.8665 ===\n",
      "====== New best is 120, Loss: 0.00015983, Acc: 99.8665 ======\n",
      "Train Epoch: 121, Loss: 0.00320474, cost: 34.91 s\n",
      "Train Epoch: 122, Loss: 0.00303482, cost: 31.43 s\n",
      "Train Epoch: 123, Loss: 0.00303735, cost: 31.41 s\n",
      "Train Epoch: 124, Loss: 0.00298328, cost: 32.35 s\n",
      "Train Epoch: 125, Loss: 0.00313286, cost: 32.18 s\n",
      "Train Epoch: 126, Loss: 0.00286335, cost: 32.16 s\n",
      "Train Epoch: 127, Loss: 0.00284529, cost: 32.15 s\n",
      "Train Epoch: 128, Loss: 0.00295711, cost: 32.19 s\n",
      "Train Epoch: 129, Loss: 0.00265814, cost: 32.17 s\n",
      "Train Epoch: 130, Loss: 0.00280755, cost: 32.19 s\n",
      "=== Test: Loss: 0.00014710, Acc: 99.8665 ===\n",
      "=== Pre best is 120, Loss: 0.00015983, Acc: 99.8665 ===\n",
      "====== New best is 130, Loss: 0.00014710, Acc: 99.8665 ======\n",
      "Train Epoch: 131, Loss: 0.00250854, cost: 35.53 s\n",
      "Train Epoch: 132, Loss: 0.00254362, cost: 32.18 s\n",
      "Train Epoch: 133, Loss: 0.00243014, cost: 32.15 s\n",
      "Train Epoch: 134, Loss: 0.00249376, cost: 32.21 s\n",
      "Train Epoch: 135, Loss: 0.00237558, cost: 32.16 s\n",
      "Train Epoch: 136, Loss: 0.00231457, cost: 32.20 s\n",
      "Train Epoch: 137, Loss: 0.00249377, cost: 32.23 s\n",
      "Train Epoch: 138, Loss: 0.00221714, cost: 32.24 s\n",
      "Train Epoch: 139, Loss: 0.00219787, cost: 32.19 s\n",
      "Train Epoch: 140, Loss: 0.00213142, cost: 32.21 s\n",
      "=== Test: Loss: 0.00013580, Acc: 99.8856 ===\n",
      "=== Pre best is 130, Loss: 0.00014710, Acc: 99.8665 ===\n",
      "====== New best is 140, Loss: 0.00013580, Acc: 99.8856 ======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m====== New best is \u001b[39m\u001b[39m{\u001b[39;00mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mbest_loss\u001b[39m:\u001b[39;00m\u001b[39m.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Acc: \u001b[39m\u001b[39m{\u001b[39;00mbest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ======\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m         torch\u001b[39m.\u001b[39msave(model, best_model_path)\n\u001b[1;32m---> 38\u001b[0m pipeline()\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(start_epoch, test_interval)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mglobal\u001b[39;00m best_epoch, best_loss, best_acc\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, \u001b[39m1000\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     train(epoch)\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m test_interval \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 8\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "output = Path('checkpoints')\n",
    "output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_name = model.__class__.__name__\n",
    "best_model_path = output / f'{model_name}_best.pt'\n",
    "\n",
    "best_epoch = 0\n",
    "best_loss = 100.0\n",
    "best_acc = 0.0\n",
    "default_interval = 10\n",
    "\n",
    "def pipeline(start_epoch = 0, test_interval = default_interval):\n",
    "    global best_epoch, best_loss, best_acc\n",
    "\n",
    "    for epoch in range(start_epoch, 1000):\n",
    "        train(epoch)\n",
    "        if epoch % test_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        loss, acc = test()\n",
    "        print(f'=== Pre best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ===')\n",
    "        torch.save(model, output / f'{model_name}_{epoch}.pt')\n",
    "        if loss > best_loss:\n",
    "            if epoch - best_epoch > test_interval * 10:\n",
    "                print('No improvement for a long time, Early stop!')\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        best_epoch = epoch\n",
    "        best_loss = loss\n",
    "        best_acc = acc\n",
    "        print(f'====== New best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ======')\n",
    "        torch.save(model, best_model_path)\n",
    "\n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: Loss: 0.00013580, Acc: 99.8856 ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00013580059955531657, 99.88558352402745)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(best_model_path)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出 onnx\n",
    "\n",
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def convert_onnx(path: Path):\n",
    "    model = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 96, 96)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        path.with_suffix(\".onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "convert_onnx(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.checker.check_model(str(best_model_path.with_suffix(\".onnx\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
