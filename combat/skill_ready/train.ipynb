{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整体流程\n",
    "\n",
    "1. 先挑选出少量数据，手工清洗一下，需要保证数据集正确率 100%\n",
    "2. 用这些数据训练一个初步模型出来\n",
    "3. 用初步模型验证整个数据集，把错误的都挑出来，再次手工清洗\n",
    "4. 用整个清洗过的数据集，重新训练一个完全体的模型\n",
    "\n",
    "下面的执行代码只包含步骤 4，前面的步骤可以自己研究下（部分代码在最后的注释里）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理原始数据集\n",
    "\n",
    "把数据集放到 datases/raw/ 目录下，建两个文件夹 y 和 n\n",
    "\n",
    "- datasets/raw/y/ 里面放技能 ready 的图\n",
    "- datasets/raw/n/ 里面放没技能的图\n",
    "\n",
    "文件名随便\n",
    "\n",
    "\n",
    "没有 gpu 的把后面代码里所有的 .cuda() 去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class SkillDataset(Dataset):\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        super().__init__()\n",
    "        self.y = list((path / 'y').glob('**/*'))\n",
    "        self.n = list((path / 'n').glob('**/*'))\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.loader = default_loader\n",
    "        # 技能图片没多大，一次性全部载入内存算了\n",
    "        self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y) + len(self.n)\n",
    "    \n",
    "    def get(self, index):\n",
    "        if index < len(self.y):\n",
    "            if index % 100 == 0:\n",
    "                print(f'load y: {index} / {len(self.y)}')\n",
    "            path = self.y[index]\n",
    "            label = 1\n",
    "        else:\n",
    "            if index % 100 == 0:\n",
    "                print(f'load n: {index - len(self.y)} / {len(self.n)}')\n",
    "            path = self.n[index - len(self.y)]\n",
    "            label = 0\n",
    "        image = self.loader(path)\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_path = Path(\"datasets/raw\")\n",
    "dataset = SkillDataset(raw_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 显存不够可以把 batch size 改小点\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_ch) -> None:\n",
    "        super().__init__()\n",
    "        self.branch_1x1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "\n",
    "        self.branch_5x5_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch_3x3_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = torch.nn.Conv2d(in_ch, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_1x1 = self.branch_1x1(x)\n",
    "\n",
    "        branch_5x5 = self.branch_5x5_1(x)\n",
    "        branch_5x5 = self.branch_5x5_2(branch_5x5)\n",
    "\n",
    "        branch_3x3 = self.branch_3x3_1(x)\n",
    "        branch_3x3 = self.branch_3x3_2(branch_3x3)\n",
    "        branch_3x3 = self.branch_3x3_3(branch_3x3)\n",
    "\n",
    "        branch_pool = torch.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch_1x1, branch_5x5, branch_3x3, branch_pool] # 16 + 24 + 24 + 24\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(88, 20, kernel_size=5)\n",
    "        self.incep1 = InceptionA(10)\n",
    "        self.incep2 = InceptionA(20)\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(32912, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incep1(x)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def train(epoch):\n",
    "    global start_time\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            cur_time = time.time()\n",
    "            duration = cur_time - start_time\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tcost: {:.2f} s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), duration))\n",
    "            start_time = cur_time\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'loss: {test_loss}', f'acc: {acc}')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    print('hello')\n",
    "    min_loss = 100.0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(1, 500):\n",
    "        train(epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            loss = test()\n",
    "            print('pre best is', best_epoch, min_loss)\n",
    "            torch.save(model, f'checkpoints/model_{epoch}.pt')\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                best_epoch = epoch\n",
    "                print('new best', best_epoch, min_loss)\n",
    "                torch.save(model, 'checkpoints/best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.356194166443271e-06 acc: 99.92270830112845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.356194166443271e-06"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出 onnx\n",
    "\n",
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def convert_onnx(path: Path):\n",
    "    model = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 100, 80)\n",
    "    axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        path.with_suffix(\".onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes=axes,\n",
    "    )\n",
    "\n",
    "\n",
    "convert_onnx(Path(\"checkpoints/best.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.checker.check_model(\"checkpoints/best.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下是部分清洗数据的代码，可以自己研究下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数据挑选，请手工清洗\n",
    "\n",
    "# from pathlib import Path\n",
    "# import random\n",
    "\n",
    "# clean_set_size = 1000\n",
    "# raw_path = Path(\"datasets/raw\")\n",
    "# positive_set = random.sample(list((raw_path / \"y\").glob(\"**/*\")), clean_set_size)\n",
    "# negative_set = random.sample(list((raw_path / \"n\").glob(\"**/*\")), clean_set_size)\n",
    "\n",
    "# clean = Path(\"datasets/clean/\")\n",
    "# clean_y = clean / \"y\"\n",
    "# clean_y.mkdir(parents=True, exist_ok=True)\n",
    "# clean_n = clean / \"n\"\n",
    "# clean_n.mkdir(parents=True, exist_ok=True)\n",
    "# for path in positive_set:\n",
    "#     path.rename(clean_y / path.name)\n",
    "# for path in negative_set:\n",
    "#     path.rename(clean_n / path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SkillRawDataset(Dataset):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.y = list(Path('datasets/raw/y/').glob('**/*'))\n",
    "#         self.n = list(Path('datasets/raw/n/').glob('**/*'))\n",
    "#         self.transform = transforms.ToTensor()\n",
    "#         self.loader = default_loader\n",
    "#         self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.y) + len(self.n)\n",
    "    \n",
    "#     def get(self, index):\n",
    "#         # print(f'load: {self.count} / {len(self)}')\n",
    "#         if index < len(self.y):\n",
    "#             path = self.y[index]\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             path = self.n[index - len(self.y)]\n",
    "#             label = 0\n",
    "#         image = self.loader(path)\n",
    "#         image = self.transform(image)\n",
    "#         return image, label\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index]\n",
    "    \n",
    "#     def get_path(self, index):\n",
    "#         if index < len(self.y):\n",
    "#             path = self.y[index]\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             path = self.n[index - len(self.y)]\n",
    "#             label = 0\n",
    "#         return path, label\n",
    "\n",
    "# raw_data_set = SkillRawDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32342\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_20-52-18.867_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_20-52-19.264_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_20-52-19.654_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_21-14-53.013_3453_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_21-32-42.363_7610_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_21-33-17.238_7986_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-23-08.921_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-33-51.125_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-33-56.957_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-34-07.050_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-34-09.139_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-34-11.213_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-34-39.334_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-34-43.518_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/y/2022-10-28_22-35-22.577_raw.png'), 1)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_20-51-10.853_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_20-58-52.640_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_20-59-10.785_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-07-39.894_1412_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-13-57.111_3084_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-02.439_3132_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-08.896_3186_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-39.651_3355_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-40.789_3367_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-41.956_3383_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-43.093_3393_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-43.098_3395_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-44.272_3407_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-54.192_3469_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-54.788_3474_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-14-55.659_3484_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-15-12.950_3626_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-15-16.459_3680_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-15-32.267_3819_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-15-35.427_3855_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-18-26.488_4487_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-18-44.708_4689_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-18-49.031_4762_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-19-23.706_5016_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-19-23.966_5019_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-26-53.163_6008_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-32-00.160_7120_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-32-00.729_7124_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-32-12.170_7252_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-32-55.598_7728_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-35-54.737_8778_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-36-00.126_8843_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-36-01.516_8858_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-44-32.508_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-44-32.883_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-44-50.439_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-44-57.566_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-45-00.275_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-45-19.829_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-51-50.815_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-51-53.233_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-51-55.629_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-51-58.137_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-52-53.864_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-57-13.349_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-57-32.173_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-57-36.602_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_21-57-41.331_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-05-04.573_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-05-47.819_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-05-59.818_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-05.673_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-06.940_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-07.355_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-08.613_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-11.119_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-29.172_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-06-39.764_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-13-20.924_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-13-59.090_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-13-59.504_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-19-11.505_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-19-22.177_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-19-22.589_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-19-22.976_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-19-23.357_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-25-08.930_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-29-26.006_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-30-07.501_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-30-08.710_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-30-30.872_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-30-39.227_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-31-00.417_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-28.534_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-28.536_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-34.904_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-39.982_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-40.844_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-41.737_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-51.131_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-54.891_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-58.572_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-33-59.397_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-00.649_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-09.547_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-15.272_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-22.034_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-25.387_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-31.348_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-42.715_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-43.938_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-47.333_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-47.763_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-47.765_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-49.442_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-49.447_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-50.268_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-54.016_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-54.459_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-34-57.789_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-16.493_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-22.180_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-22.586_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-23.390_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-23.810_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-35-24.210_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-37-41.245_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-37-57.435_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-38-00.352_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-38-27.651_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-38-32.523_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-38-39.131_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-38-50.337_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-39-01.712_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-39-02.558_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-39-15.765_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-39-34.641_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-42-38.364_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-42-54.802_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-42-56.108_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-42-59.742_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-00.123_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-00.523_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-00.938_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-01.335_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-01.758_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-02.175_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-02.602_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-03.047_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-03.908_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-04.328_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-04.753_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-05.165_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-08.457_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-08.865_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-09.278_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-09.721_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-10.167_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-10.594_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-10.598_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-11.000_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-11.430_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-12.310_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-12.741_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-13.187_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-13.623_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-14.061_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-14.510_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-14.954_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-15.382_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-15.812_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-16.236_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-16.660_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-16.664_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-19.864_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-20.277_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-20.736_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-21.161_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-21.585_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-21.998_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-22.421_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-22.910_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-23.352_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-23.810_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-24.231_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-24.695_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-25.162_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-25.166_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-25.586_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-26.009_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-26.444_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-26.907_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-27.310_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-27.742_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-28.172_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-28.579_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-29.008_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-30.720_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-35.324_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-35.742_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-43.051_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-43-43.452_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-44-02.666_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-44-04.289_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-44-06.786_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-44-09.418_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-46-57.517_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-03.813_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-06.760_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-25.866_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-25.867_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-29.056_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-32.160_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-35.491_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-47-59.156_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-52-09.278_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-52-43.583_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-52-45.628_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-53-01.016_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-53-07.097_raw.png'), 0)\n",
      "(WindowsPath('datasets/raw/n/2022-10-28_22-53-11.446_raw.png'), 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# raw_loader = DataLoader(raw_data_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "# import os\n",
    "# import shutil\n",
    "# def clear():\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     Path('datasets/maybe_error/1').mkdir(parents=True, exist_ok=True)\n",
    "#     Path('datasets/maybe_error/0').mkdir(parents=True, exist_ok=True)\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (data, target) in enumerate(raw_loader):\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target).item() # sum up batch loss\n",
    "#             test_loss += loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "#             correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "#             if not correct:\n",
    "#                 tup = raw_data_set.get_path(batch_idx)\n",
    "#                 print(tup)\n",
    "#                 os.rename(tup[0], Path('datasets/maybe_error/') / str(tup[1]) / tup[0].name)                \n",
    "                \n",
    "\n",
    "\n",
    "# print(len(raw_data_set))\n",
    "# clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
