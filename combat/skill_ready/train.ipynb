{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理数据集\n",
    "\n",
    "把数据集放到 datases/raw/ 目录下，建两个文件夹 y 和 n\n",
    "\n",
    "- datasets/raw/y/ 里面放技能 ready 的图\n",
    "- datasets/raw/n/ 里面放没技能的图\n",
    "\n",
    "要求输入图片尺寸为 3x64x64，文件名随便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class SkillDataset(Dataset):\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        super().__init__()\n",
    "        self.y = list((path / 'y').glob('**/*'))\n",
    "        self.n = list((path / 'n').glob('**/*'))\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.loader = default_loader\n",
    "        # 技能图片没多大，一次性全部载入内存算了\n",
    "        self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y) + len(self.n)\n",
    "    \n",
    "    def get(self, index):\n",
    "        if index < len(self.y):\n",
    "            if index % 100 == 0:\n",
    "                print(f'load y: {index} / {len(self.y)}')\n",
    "            path = self.y[index]\n",
    "            label = 1\n",
    "        else:\n",
    "            if index % 100 == 0:\n",
    "                print(f'load n: {index - len(self.y)} / {len(self.n)}')\n",
    "            path = self.n[index - len(self.y)]\n",
    "            label = 0\n",
    "        image = self.loader(path)\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "raw_path = Path(\"datasets/raw\")\n",
    "dataset = SkillDataset(raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 显存不够可以把 batch size 改小点\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_ch) -> None:\n",
    "        super().__init__()\n",
    "        self.branch_1x1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "\n",
    "        self.branch_5x5_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch_3x3_1 = torch.nn.Conv2d(in_ch, 16, kernel_size=1)\n",
    "        self.branch_3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = torch.nn.Conv2d(in_ch, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_1x1 = self.branch_1x1(x)\n",
    "\n",
    "        branch_5x5 = self.branch_5x5_1(x)\n",
    "        branch_5x5 = self.branch_5x5_2(branch_5x5)\n",
    "\n",
    "        branch_3x3 = self.branch_3x3_1(x)\n",
    "        branch_3x3 = self.branch_3x3_2(branch_3x3)\n",
    "        branch_3x3 = self.branch_3x3_3(branch_3x3)\n",
    "\n",
    "        branch_pool = torch.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch_1x1, branch_5x5, branch_3x3, branch_pool] # 16 + 24 + 24 + 24\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class GoogleNet(torch.nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(channels, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(88, 20, kernel_size=5)\n",
    "        self.incep1 = InceptionA(10)\n",
    "        self.incep2 = InceptionA(20)\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(14872, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incep1(x)\n",
    "        x = torch.nn.functional.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if not use_cuda:\n",
    "    print(\"WARNING: CPU will be used for training.\")\n",
    "\n",
    "model = GoogleNet(3).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def train(epoch):\n",
    "    global start_time\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    cur_time = time.time()\n",
    "    cost = cur_time - start_time\n",
    "    print(f'Train Epoch: {epoch}, Loss: {loss.item():.8f}, cost: {cost:.2f} s')\n",
    "    start_time = cur_time\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'=== Test: Loss: {test_loss:.8f}, Acc: {acc:.4f} ===')\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "output = Path('checkpoints')\n",
    "output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "best_epoch = 0\n",
    "best_loss = 100.0\n",
    "best_acc = 0.0\n",
    "default_interval = 10\n",
    "\n",
    "def pipeline(start_epoch = 0, test_interval = default_interval):\n",
    "    global best_epoch, best_loss, best_acc\n",
    "\n",
    "    for epoch in range(start_epoch, 1000):\n",
    "        train(epoch)\n",
    "        if epoch % test_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        loss, acc = test()\n",
    "        print(f'=== Pre best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ===')\n",
    "        torch.save(model, output / f'model_{epoch}.pt')\n",
    "        if loss > best_loss:\n",
    "            if epoch - best_epoch > test_interval * 10:\n",
    "                print('No improvement for a long time, Early stop!')\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        best_epoch = epoch\n",
    "        best_loss = loss\n",
    "        best_acc = acc\n",
    "        print(f'====== New best is {best_epoch}, Loss: {best_loss:.8f}, Acc: {best_acc:.4f} ======')\n",
    "        torch.save(model, output / 'best.pt')\n",
    "\n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(output / f'best.pt')\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出 onnx\n",
    "\n",
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def convert_onnx(path: Path):\n",
    "    model = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 64, 64)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        path.with_suffix(\".onnx\"),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "convert_onnx(output / \"best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.checker.check_model(str(output / \"best.onnx\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下是部分清洗数据的代码，可以自己研究下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数据挑选，请手工清洗\n",
    "\n",
    "# from pathlib import Path\n",
    "# import random\n",
    "\n",
    "# clean_set_size = 1000\n",
    "# raw_path = Path(\"datasets/raw\")\n",
    "# positive_set = random.sample(list((raw_path / \"y\").glob(\"**/*\")), clean_set_size)\n",
    "# negative_set = random.sample(list((raw_path / \"n\").glob(\"**/*\")), clean_set_size)\n",
    "\n",
    "# clean = Path(\"datasets/clean/\")\n",
    "# clean_y = clean / \"y\"\n",
    "# clean_y.mkdir(parents=True, exist_ok=True)\n",
    "# clean_n = clean / \"n\"\n",
    "# clean_n.mkdir(parents=True, exist_ok=True)\n",
    "# for path in positive_set:\n",
    "#     path.rename(clean_y / path.name)\n",
    "# for path in negative_set:\n",
    "#     path.rename(clean_n / path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SkillRawDataset(Dataset):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.y = list(Path('datasets/raw/y/').glob('**/*'))\n",
    "#         self.n = list(Path('datasets/raw/n/').glob('**/*'))\n",
    "#         self.transform = transforms.ToTensor()\n",
    "#         self.loader = default_loader\n",
    "#         self.data = [ self.get(i) for i in range(len(self))]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.y) + len(self.n)\n",
    "    \n",
    "#     def get(self, index):\n",
    "#         # print(f'load: {self.count} / {len(self)}')\n",
    "#         if index < len(self.y):\n",
    "#             path = self.y[index]\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             path = self.n[index - len(self.y)]\n",
    "#             label = 0\n",
    "#         image = self.loader(path)\n",
    "#         image = self.transform(image)\n",
    "#         return image, label\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index]\n",
    "    \n",
    "#     def get_path(self, index):\n",
    "#         if index < len(self.y):\n",
    "#             path = self.y[index]\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             path = self.n[index - len(self.y)]\n",
    "#             label = 0\n",
    "#         return path, label\n",
    "\n",
    "# raw_data_set = SkillRawDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw_loader = DataLoader(raw_data_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "# import os\n",
    "# import shutil\n",
    "# def clear():\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     Path('datasets/maybe_error/1').mkdir(parents=True, exist_ok=True)\n",
    "#     Path('datasets/maybe_error/0').mkdir(parents=True, exist_ok=True)\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (data, target) in enumerate(raw_loader):\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target).item() # sum up batch loss\n",
    "#             test_loss += loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "#             correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "#             if not correct:\n",
    "#                 tup = raw_data_set.get_path(batch_idx)\n",
    "#                 print(tup)\n",
    "#                 os.rename(tup[0], Path('datasets/maybe_error/') / str(tup[1]) / tup[0].name)                \n",
    "                \n",
    "\n",
    "\n",
    "# print(len(raw_data_set))\n",
    "# clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
